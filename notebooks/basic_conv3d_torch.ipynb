{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building submodels for Convnet\n",
    "#### Linear Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _LinearBlock(nn.Module):\n",
    "    # Basic Linear Block\n",
    "\n",
    "    def __init__(self, inFeatures, outFeatures, act=\"relu\", flatten=False, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define activation function to be used in this block\n",
    "        match act:\n",
    "            case \"identity\":\n",
    "                activationFunction = nn.Identity()\n",
    "            case \"relu\":\n",
    "                activationFunction = nn.ReLU()\n",
    "            case \"leaky\":\n",
    "                activationFunction = nn.LeakyReLU()\n",
    "            case \"gelu\":\n",
    "                activationFunction = nn.GELU()\n",
    "            case \"sigmoid\":\n",
    "                activationFunction = nn.Sigmoid()\n",
    "            case _:\n",
    "                raise Exception(f\"{act} is not a recognised activation function for this class\")\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1) if flatten else nn.Identity(), \n",
    "            nn.Linear(\n",
    "                inFeatures,\n",
    "                outFeatures,\n",
    "                **kwargs,\n",
    "            ),\n",
    "            nn.BatchNorm1d(outFeatures),\n",
    "            activationFunction,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Output result of conv block when object is called\n",
    "\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ConvBlock(nn.Module):\n",
    "    # Basic convolutional block\n",
    "\n",
    "    def __init__(self, inChannels, outChannels, down=True, act=\"relu\", **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define activation function to be used in this block\n",
    "        match act:\n",
    "            case \"identity\":\n",
    "                activationFunction = nn.Identity()\n",
    "            case \"relu\":\n",
    "                activationFunction = nn.ReLU()\n",
    "            case \"leaky\":\n",
    "                activationFunction = nn.LeakyReLU()\n",
    "            case \"gelu\":\n",
    "                activationFunction = nn.GELU()\n",
    "            case _:\n",
    "                raise Exception(f\"{act} is not a recognised activation function for this class\")\n",
    "\n",
    "        # Define generic convolutional block / transpose convolutional block\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(\n",
    "                inChannels, \n",
    "                outChannels,\n",
    "                padding_mode=\"reflect\",\n",
    "                **kwargs,\n",
    "            ) \n",
    "            if down else nn.ConvTranspose3d(\n",
    "                inChannels,\n",
    "                outChannels,\n",
    "                **kwargs,\n",
    "            ),\n",
    "            nn.BatchNorm3d(outChannels),\n",
    "            activationFunction,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Output result of conv block when object is called\n",
    "\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non Downsampling Residual block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ResidualBlock(nn.Module):\n",
    "    # Basic Residual block\n",
    "\n",
    "    def __init__(self,channels, act=\"leaky\"):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define convolutional blocks in residual blocks\n",
    "        self.resBlock = nn.Sequential(\n",
    "            _ConvBlock(\n",
    "                channels, \n",
    "                channels,\n",
    "                act=act,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            _ConvBlock(\n",
    "                channels,\n",
    "                channels,\n",
    "                act=\"identity\",\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Define operations to be made to input when object is called\n",
    "        \n",
    "        return x + self.resBlock(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  DownSampling Residual Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _DownsampleResidualBlock(nn.Module):\n",
    "    # Basic Residual block\n",
    "\n",
    "    def __init__(self, inChannels, outChannels, act=\"leaky\"):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define convolutional blocks in residual blocks\n",
    "        self.resBlockDown = nn.Sequential(\n",
    "            _ConvBlock(\n",
    "                inChannels, \n",
    "                outChannels,\n",
    "                act=act,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1\n",
    "            ),\n",
    "            _ConvBlock(\n",
    "                outChannels, \n",
    "                outChannels,\n",
    "                act=\"identity\",\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Define normal downsample convolution for resudial calculation in input is downsamples\n",
    "        self.resBlockDownSkip = nn.Sequential(\n",
    "            _ConvBlock(\n",
    "                inChannels,\n",
    "                outChannels,\n",
    "                act= \"identity\",\n",
    "                kernel_size=1,\n",
    "                stride=2,\n",
    "                padding=0\n",
    "            ),      \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Define operations to be made to input when object is called\n",
    "        \n",
    "        return self.resBlockDownSkip(x) + self.resBlockDown(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### :warning: No activation function in output of resblock as described in original paper. :warning:\n",
    "Maybe change in future using relu as suggested in original paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test layer outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 128, 128, 128])\n",
      "1 10 128 128 128\n",
      "\n",
      "\n",
      "torch.Size([1, 20, 64, 64, 64])\n",
      "1 20 64.0 64.0 64.0\n",
      "\n",
      "\n",
      "torch.Size([1, 10, 64, 64, 64])\n",
      "1 10 64.0 64.0 64.0\n",
      "\n",
      "\n",
      "torch.Size([1, 20, 128, 128, 128])\n",
      "1 20 128 128 128\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "N = 128\n",
    "C = 10\n",
    "layer1 = _ResidualBlock(C, act=\"leaky\").to(DEVICE)\n",
    "layer2 = _DownsampleResidualBlock(C, C*2, act=\"leaky\").to(DEVICE)\n",
    "layer3 = nn.MaxPool3d(kernel_size=3, stride=2, padding=1,).to(DEVICE)\n",
    "layer4 = _ConvBlock(\n",
    "            C,\n",
    "            C*2,\n",
    "            act=\"leaky\",\n",
    "            kernel_size=7,\n",
    "            padding=3,\n",
    "            bias=False,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "input = torch.randn((1, C, N, N, N), dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "# Outputs should be the same\n",
    "output = layer1(input)\n",
    "print(output.shape)\n",
    "print(1, C, N, N, N)\n",
    "\n",
    "print(\"\\n\")\n",
    "# Outputs should be the same\n",
    "output = layer2(input)\n",
    "print(output.shape)\n",
    "print(1, C*2, N/2, N/2, N/2)\n",
    "\n",
    "print(\"\\n\")\n",
    "# Outputs should be the same\n",
    "output = layer3(input)\n",
    "print(output.shape)\n",
    "print(1, C, N/2, N/2, N/2)\n",
    "\n",
    "print(\"\\n\")\n",
    "# Outputs should be the same\n",
    "output = layer4(input)\n",
    "print(output.shape)\n",
    "print(1, C*2, N, N, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Multiclass Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier3D(nn.Module):\n",
    "    def __init__(self, imageChannels, numFeatures=64, listResiduals=[3, 4, 6, 3]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define initial block of the analiser\n",
    "        self.initialLayer = nn.Sequential(\n",
    "            _ConvBlock(\n",
    "                imageChannels,\n",
    "                numFeatures,\n",
    "                act=\"leaky\",\n",
    "                kernel_size=7,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm3d(numFeatures),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool3d(kernel_size=3, stride=2, padding=1,),\n",
    "        )\n",
    "\n",
    "        # Define first residual block with 64 features\n",
    "        # Note: we use actual number in list as there is no downsample res block\n",
    "        resBlock64 = nn.Sequential(\n",
    "            *[_ResidualBlock(numFeatures, act=\"leaky\") for _ in range(listResiduals[0])],\n",
    "        )\n",
    "\n",
    "        # Define second residual block with 128 features\n",
    "        resBlock128 = nn.Sequential(\n",
    "            _DownsampleResidualBlock(numFeatures, numFeatures*2, act=\"leaky\"),\n",
    "            *[_ResidualBlock(numFeatures*2, act=\"leaky\") for _ in range(listResiduals[1]-1)],\n",
    "        )\n",
    "\n",
    "        # Define third residual block with 256 features\n",
    "        resBlock256 = nn.Sequential(\n",
    "            _DownsampleResidualBlock(numFeatures*2, numFeatures*4, act=\"leaky\"),\n",
    "            *[_ResidualBlock(numFeatures*4, act=\"leaky\") for _ in range(listResiduals[2]-1)],\n",
    "        )\n",
    "\n",
    "        # Define third residual block with 512 features\n",
    "        resBlock512 = nn.Sequential(\n",
    "            _DownsampleResidualBlock(numFeatures*4, numFeatures*8, act=\"leaky\"),\n",
    "            *[_ResidualBlock(numFeatures*8, act=\"leaky\") for _ in range(listResiduals[3]-1)],\n",
    "        )\n",
    "\n",
    "        self.resBlocksAll = nn.ModuleList([\n",
    "            resBlock64,\n",
    "            resBlock128,\n",
    "            resBlock256,\n",
    "            resBlock512\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.initialLayer(x)\n",
    "        print(\"layer 1 done\")\n",
    "\n",
    "        for idx, layer in enumerate(self.resBlocksAll):\n",
    "            x = layer(x)\n",
    "            print(idx, x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test classifier outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1 done\n",
      "0 torch.Size([1, 64, 61, 61, 61])\n",
      "1 torch.Size([1, 128, 31, 31, 31])\n",
      "2 torch.Size([1, 256, 16, 16, 16])\n",
      "3 torch.Size([1, 512, 8, 8, 8])\n",
      "torch.Size([1, 512, 8, 8, 8])\n",
      "1 512 8.0 8.0 8.0\n"
     ]
    }
   ],
   "source": [
    "N = 128\n",
    "C = 1\n",
    "image3D = torch.randn((1, C, N, N, N)).to(DEVICE)\n",
    "classifier = Classifier3D(C, numFeatures=64, listResiduals=[3, 4, 6, 3]).to(DEVICE)\n",
    "\n",
    "# Outputs should be the same\n",
    "output = classifier(image3D)\n",
    "print(output.shape)\n",
    "print(1, 512, N/16, N/16, N/16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
