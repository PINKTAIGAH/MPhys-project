{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from math import prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building submodels for Convnet\n",
    "#### Linear Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _LinearBlock(nn.Module):\n",
    "    # Basic Linear Block\n",
    "\n",
    "    def __init__(self, inFeatures, outFeatures, act=\"relu\", flatten=False, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define activation function to be used in this block\n",
    "        match act:\n",
    "            case \"identity\":\n",
    "                activationFunction = nn.Identity()\n",
    "            case \"relu\":\n",
    "                activationFunction = nn.ReLU()\n",
    "            case \"leaky\":\n",
    "                activationFunction = nn.LeakyReLU()\n",
    "            case \"gelu\":\n",
    "                activationFunction = nn.GELU()\n",
    "            case \"sigmoid\":\n",
    "                activationFunction = nn.Sigmoid()\n",
    "            case _:\n",
    "                raise Exception(f\"{act} is not a recognised activation function for this class\")\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1) if flatten else nn.Identity(), \n",
    "            nn.Linear(\n",
    "                inFeatures,\n",
    "                outFeatures,\n",
    "                **kwargs,\n",
    "            ),\n",
    "            activationFunction,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Output result of conv block when object is called\n",
    "\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ConvBlock(nn.Module):\n",
    "    # Basic convolutional block\n",
    "\n",
    "    def __init__(self, inChannels, outChannels, down=True, act=\"relu\", batchnorm=True, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define activation function to be used in this block\n",
    "        match act:\n",
    "            case \"identity\":\n",
    "                activationFunction = nn.Identity()\n",
    "            case \"relu\":\n",
    "                activationFunction = nn.ReLU()\n",
    "            case \"leaky\":\n",
    "                activationFunction = nn.LeakyReLU()\n",
    "            case \"gelu\":\n",
    "                activationFunction = nn.GELU()\n",
    "            case _:\n",
    "                raise Exception(f\"{act} is not a recognised activation function for this class\")\n",
    "\n",
    "        # Define generic convolutional block / transpose convolutional block\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(\n",
    "                inChannels, \n",
    "                outChannels,\n",
    "                padding_mode=\"reflect\",\n",
    "                **kwargs,\n",
    "            ) \n",
    "            if down else nn.ConvTranspose3d(\n",
    "                inChannels,\n",
    "                outChannels,\n",
    "                **kwargs,\n",
    "            ),\n",
    "            nn.BatchNorm3d(outChannels) if batchnorm else nn.Identity(),\n",
    "            activationFunction,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Output result of conv block when object is called\n",
    "\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non Downsampling Residual block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ResidualBlock(nn.Module):\n",
    "    # Basic Residual block\n",
    "\n",
    "    def __init__(self,channels, act=\"leaky\", batchnorm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define convolutional blocks in residual blocks\n",
    "        self.resBlock = nn.Sequential(\n",
    "            _ConvBlock(\n",
    "                channels, \n",
    "                channels,\n",
    "                act=act,\n",
    "                batchnorm=batchnorm,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            _ConvBlock(\n",
    "                channels,\n",
    "                channels,\n",
    "                act=\"identity\",\n",
    "                batchnorm=batchnorm,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Define operations to be made to input when object is called\n",
    "        \n",
    "        return x + self.resBlock(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  DownSampling Residual Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _DownsampleResidualBlock(nn.Module):\n",
    "    # Basic Residual block\n",
    "\n",
    "    def __init__(self, inChannels, outChannels, act=\"leaky\", batchnorm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define convolutional blocks in residual blocks\n",
    "        self.resBlockDown = nn.Sequential(\n",
    "            _ConvBlock(\n",
    "                inChannels, \n",
    "                outChannels,\n",
    "                act=act,\n",
    "                batchnorm=batchnorm,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1\n",
    "            ),\n",
    "            _ConvBlock(\n",
    "                outChannels, \n",
    "                outChannels,\n",
    "                act=\"identity\",\n",
    "                batchnorm=batchnorm,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Define normal downsample convolution for resudial calculation in input is downsamples\n",
    "        self.resBlockDownSkip = nn.Sequential(\n",
    "            _ConvBlock(\n",
    "                inChannels,\n",
    "                outChannels,\n",
    "                act= \"identity\",\n",
    "                batchnorm=True,\n",
    "                kernel_size=1,\n",
    "                stride=2,\n",
    "                padding=0\n",
    "            ),      \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Define operations to be made to input when object is called\n",
    "        \n",
    "        return self.resBlockDownSkip(x) + self.resBlockDown(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### :warning: No activation function in output of resblock as described in original paper. :warning:\n",
    "Maybe change in future using relu as suggested in original paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test layer outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "N = 128\n",
    "C = 10\n",
    "layer1 = _ResidualBlock(C, act=\"leaky\").to(DEVICE)\n",
    "layer2 = _DownsampleResidualBlock(C, C*2, act=\"leaky\").to(DEVICE)\n",
    "layer3 = nn.MaxPool3d(kernel_size=3, stride=2, padding=1,).to(DEVICE)\n",
    "layer4 = _ConvBlock(\n",
    "            C,\n",
    "            C*2,\n",
    "            act=\"leaky\",\n",
    "            batchnorm=True,\n",
    "            kernel_size=7,\n",
    "            stride=2,\n",
    "            padding=3,\n",
    "            bias=False,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "input = torch.randn((1, C, N, N, N), dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "# Outputs should be the same\n",
    "output = layer1(input)\n",
    "print(output.shape)\n",
    "print(1, C, N, N, N)\n",
    "\n",
    "print(\"\\n\")\n",
    "# Outputs should be the same\n",
    "output = layer2(input)\n",
    "print(output.shape)\n",
    "print(1, C*2, N/2, N/2, N/2)\n",
    "\n",
    "print(\"\\n\")\n",
    "# Outputs should be the same\n",
    "output = layer3(input)\n",
    "print(output.shape)\n",
    "print(1, C, N/2, N/2, N/2)\n",
    "\n",
    "print(\"\\n\")\n",
    "# Outputs should be the same\n",
    "output = layer4(input)\n",
    "print(output.shape)\n",
    "print(1, C*2, N/2, N/2, N/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Multiclass Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier3D(nn.Module):\n",
    "    def __init__(self, imageChannels, numClasses, imageDimentions=(64, 64, 64),\n",
    "                numFeatures=64, listResiduals=[3, 4, 6, 3]):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Define initial block of the analiser\n",
    "        self.initialLayer = nn.Sequential(\n",
    "            _ConvBlock(\n",
    "                imageChannels,\n",
    "                numFeatures,\n",
    "                act=\"leaky\",\n",
    "                batchnorm=True,\n",
    "                kernel_size=7,\n",
    "                stride=2,\n",
    "                padding=3,               \n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm3d(numFeatures),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool3d(kernel_size=3, stride=2, padding=1,),\n",
    "        )\n",
    "\n",
    "        # Define first residual block with 64 features\n",
    "        # Note: we use actual number in list as there is no downsample res block\n",
    "        resBlock64 = nn.Sequential(\n",
    "            *[_ResidualBlock(numFeatures, act=\"leaky\", batchnorm=True) for _ in range(listResiduals[0])],\n",
    "        )\n",
    "\n",
    "        # Define second residual block with 128 features\n",
    "        resBlock128 = nn.Sequential(\n",
    "            _DownsampleResidualBlock(numFeatures, numFeatures*2, act=\"leaky\"),\n",
    "            *[_ResidualBlock(numFeatures*2, act=\"leaky\", batchnorm=True) for _ in range(listResiduals[1]-1)],\n",
    "        )\n",
    "\n",
    "        # Define third residual block with 256 features\n",
    "        resBlock256 = nn.Sequential(\n",
    "            _DownsampleResidualBlock(numFeatures*2, numFeatures*4, act=\"leaky\"),\n",
    "            *[_ResidualBlock(numFeatures*4, act=\"leaky\", batchnorm=True) for _ in range(listResiduals[2]-1)],\n",
    "        )\n",
    "\n",
    "        # Define third residual block with 512 features\n",
    "        resBlock512 = nn.Sequential(\n",
    "            _DownsampleResidualBlock(numFeatures*4, numFeatures*8, act=\"leaky\"),\n",
    "            _ResidualBlock(numFeatures*8, act=\"leaky\", batchnorm=True),\n",
    "            _ResidualBlock(numFeatures*8, act=\"leaky\", batchnorm=False)\n",
    "        )\n",
    "\n",
    "        self.resBlocksAll = nn.ModuleList([\n",
    "            resBlock64,\n",
    "            resBlock128,\n",
    "            resBlock256,\n",
    "            resBlock512\n",
    "        ])\n",
    "\n",
    "        # We know that the hight and width of the latent tensor after all resnet is (B, 512, H/32, W/32, L/32)\n",
    "        # Define number of nodes in linear layers\n",
    "        productLatentDimentions = int((imageDimentions[0]/32) * (imageDimentions[1]/32) * (imageDimentions[2]/32))\n",
    "        flattenedInFeatures = 512*productLatentDimentions\n",
    "        \n",
    "        self.denseBlocks = nn.ModuleList([\n",
    "            _LinearBlock(\n",
    "                flattenedInFeatures,\n",
    "                numClasses,\n",
    "                act=\"identity\",\n",
    "                flatten=True,\n",
    "                bias=True\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Apply initial layer\n",
    "        x = self.initialLayer(x)        # Size: (B, 64, H/2, W/2, L/2)\n",
    "\n",
    "        # Apply all resnet layers\n",
    "        for layer in self.resBlocksAll:\n",
    "            x = layer(x)\n",
    "        # Size: (B, 512, H/32, W/32, L/32)\n",
    "        \n",
    "        # Apply linear layers \n",
    "        for layer in self.denseBlocks:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version of Resnet for MedMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierMed(nn.Module):\n",
    "    def __init__(self, imageChannels, numClasses, imageDimentions=(64, 64, 64),\n",
    "                numFeatures=64, listResiduals=[3, 8,]):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Define initial block of the analiser\n",
    "        self.initialLayer = nn.Sequential(\n",
    "            _ConvBlock(\n",
    "                imageChannels,\n",
    "                numFeatures,\n",
    "                act=\"leaky\",\n",
    "                batchnorm=True,\n",
    "                kernel_size=7,\n",
    "                stride=2,\n",
    "                padding=3,               \n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm3d(numFeatures),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool3d(kernel_size=3, stride=2, padding=1,),\n",
    "        )\n",
    "\n",
    "        # Define first residual block with 64 features\n",
    "        # Note: we use actual number in list as there is no downsample res block\n",
    "        resBlock64 = nn.Sequential(\n",
    "            *[_ResidualBlock(numFeatures, act=\"leaky\", batchnorm=True) for _ in range(listResiduals[0])],\n",
    "        )\n",
    "\n",
    "        # Define second residual block with 128 features\n",
    "        resBlock128 = nn.Sequential(\n",
    "            _DownsampleResidualBlock(numFeatures, numFeatures*2, act=\"leaky\"),\n",
    "            *[_ResidualBlock(numFeatures*2, act=\"leaky\", batchnorm=True) for _ in range(listResiduals[1]-1)],\n",
    "        )\n",
    "\n",
    "        self.resBlocksAll = nn.ModuleList([\n",
    "            resBlock64,\n",
    "            resBlock128,\n",
    "        ])\n",
    "\n",
    "        # We know that the hight and width of the latent tensor after all resnet is (B, 128, H/7, W/7, L/7)\n",
    "        # Define number of nodes in linear layers\n",
    "        productLatentDimentions = int((imageDimentions[0]/7) * (imageDimentions[1]/7) * (imageDimentions[2]/7))\n",
    "        flattenedInFeatures = 128*productLatentDimentions\n",
    "        \n",
    "        self.denseBlocks = nn.ModuleList([\n",
    "            _LinearBlock(\n",
    "                flattenedInFeatures,\n",
    "                numClasses,\n",
    "                act=\"identity\",\n",
    "                flatten=True,\n",
    "                bias=True\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Apply initial layer\n",
    "        x = self.initialLayer(x)        # Size: (B, 64, H/2, W/2, L/2)\n",
    "\n",
    "        # Apply all resnet layers\n",
    "        for layer in self.resBlocksAll:\n",
    "            x = layer(x)\n",
    "        # Size: (B, 128, H/7, W/7, L/7)\n",
    "        \n",
    "        # Apply linear layers \n",
    "        for layer in self.denseBlocks:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test classifier outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 64\n",
    "C = 1\n",
    "numClasses = 2\n",
    "image3D = torch.randn((1, C, N, N, N)).to(DEVICE)\n",
    "classifier1 = Classifier3D(\n",
    "    C,\n",
    "    numFeatures=64,\n",
    "    numClasses=2, \n",
    "    imageDimentions=(N, N, N), \n",
    "    listResiduals=[3, 4, 6, 3]\n",
    "    ).to(DEVICE)\n",
    "\n",
    "M = 28\n",
    "imageMed = torch.randn((1, C, M, M, M)).to(DEVICE)\n",
    "classifier2 = ClassifierMed(\n",
    "    C,\n",
    "    numFeatures=64,\n",
    "    numClasses=2, \n",
    "    imageDimentions=(M, M, M), \n",
    "    listResiduals=[3, 8,]\n",
    "    ).to(DEVICE)\n",
    "\n",
    "# Outputs should be the same\n",
    "output = classifier1(image3D)\n",
    "print(output.shape)\n",
    "print(1, 2)\n",
    "\n",
    "# Outputs should be the same\n",
    "output = classifier2(imageMed)\n",
    "print(output.shape)\n",
    "print(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
