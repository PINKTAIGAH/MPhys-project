{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec947600",
   "metadata": {},
   "source": [
    "# Queued IO\n",
    "\n",
    "One of the main draws of ``larcv`` for machine learning is the ability to use the queued data reader.  This preloads the next batch of data (you can pick it, or you can let ``larcv`` randomly go through your dataset) while you are working on the current one.\n",
    "\n",
    "Often, the details of the IO needs of an application are specific enough that it is difficult to write a sufficiently generic interface at a high level to make this into a one-line-of-code situation.  So, this tutorial aims to show the highest level interface and how you can use it, hopefully providing enough of a starting point to move forward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91407792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import larcv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fcefac",
   "metadata": {},
   "source": [
    "\n",
    "## Step 0 - Creating an input file\n",
    "\n",
    "Obviously, skip this step when you are ready with your own input file, but here's a short step to generate an input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e3925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor2d(image_meta):\n",
    "    # Create an return a list of Tensor2D objects with dense shape as defined\n",
    "\n",
    "    for i in range(n_projection_ids):\n",
    "        data = numpy.random.uniform(size=dense_shape).astype(\"float32\")\n",
    "        tensor = larcv.Tensor2D(data)\n",
    "        # Creating from numpy automatically sets the projection ID to 0, so fix that:\n",
    "        tensor.set_projection_id(i)\n",
    "        tensor_2ds.append(tensor)\n",
    "    return tensor_2ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf108dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What should the name of the file be?\n",
    "output = \"queue_io_input.h5\"\n",
    "\n",
    "# Create an output larcv file, using the WRITE mode\n",
    "io_manager = larcv.IOManager(larcv.IOManager.kWRITE)\n",
    "io_manager.set_out_file(str(output))\n",
    "io_manager.initialize()\n",
    "\n",
    "n_events = 10\n",
    "\n",
    "n_voxels = (64, 64, 64)\n",
    "boundary_min = numpy.asarray((-50, -50, -50))\n",
    "boundary_max = numpy.asarray(( 50,  50,  50))\n",
    "\n",
    "# compute the origin:\n",
    "length = (boundary_max - boundary_min)\n",
    "\n",
    "print(length)\n",
    "# For this, we'll use one pre-defined image meta\n",
    "meta = larcv.ImageMeta3D()\n",
    "for i_axis in [0,1,2]:\n",
    "    meta.set_dimension(i_axis, length[i_axis], n_voxels[i_axis], boundary_min[i_axis])\n",
    "\n",
    "print(meta)\n",
    "\n",
    "step_size = 0.5\n",
    "\n",
    "for i in range(n_events):\n",
    "    # Create some random data as input.\n",
    "\n",
    "    # We're going to create lines of data, with random start points and directions,\n",
    "    # And fill in voxels along those lines.\n",
    "    \n",
    "    start     = boundary_min + numpy.random.uniform(size=3)*boundary_max\n",
    "    direction = numpy.random.uniform(size=3) - 0.5\n",
    "    print(direction)\n",
    "    \n",
    "    voxel_set = larcv.VoxelSet()\n",
    "    \n",
    "    p = start\n",
    "    \n",
    "    # Loop until this trajectory exits the space:\n",
    "    while (p > boundary_min).all() and (p < boundary_max).all():\n",
    "        \n",
    "        # Use ImageMeta to figure out what voxel this goes into:\n",
    "        index = meta.position_to_index(p)\n",
    "        # larcv will automatically check if this voxel already exists and overwrite with \"insert\"\n",
    "        # (Use \"add\" to add values instead)\n",
    "        voxel_set.insert(larcv.Voxel(index, value=1.0))\n",
    "        \n",
    "        p = p + step_size * direction\n",
    "#     # Now, store this set of voxels in the event:\n",
    "    event_sparse_tensor = io_manager.get_data(\"sparse3d\", \"queue_demo\")\n",
    "    event_sparse_tensor.set(voxel_set, meta)\n",
    "    \n",
    "    # Let's also save the start position and momentum using a Particle object:\n",
    "    particle = larcv.Particle()\n",
    "    particle.position(*start, 0.0)\n",
    "    particle.momentum(*direction)\n",
    "    \n",
    "    event_particle = io_manager.get_data(\"particle\", \"queue_demo\")\n",
    "    \n",
    "    event_particle.append(particle)\n",
    "    \n",
    "#     # Save the data:\n",
    "    io_manager.save_entry()\n",
    "    \n",
    "io_manager.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d32a006",
   "metadata": {},
   "source": [
    "## Step 1 - Configuring\n",
    "\n",
    "To use the Queue IO layer, we have to provide it with a proper config file.  We try to make this easy with ``larcv.config_builder``:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59eb0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the config builder and turn verbosity down low:\n",
    "from larcv.config_builder import ConfigBuilder\n",
    "cb = ConfigBuilder()\n",
    "cb.set_parameter([\"queue_io_input.h5\"], \"InputFiles\") # Pass input files as a list because you can combine them here\n",
    "cb.set_parameter(5, \"ProcessDriver\", \"IOManager\", \"Verbosity\")\n",
    "cb.set_parameter(5, \"ProcessDriver\", \"Verbosity\")\n",
    "cb.set_parameter(5, \"Verbosity\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bf02d1",
   "metadata": {},
   "source": [
    "Config Builder works by adding batch_fillers and preprocess apps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb582fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"demo\"\n",
    "\n",
    "# Bring in the wires:\n",
    "cb.add_batch_filler(\n",
    "    datatype  = \"sparse3d\",\n",
    "    producer  = \"queue_demo\",\n",
    "    name      = name+\"data\", # This is the name in the output dict to place this data\n",
    "    MaxVoxels = 1000, # Zero pad the empty voxels for this many voxels\n",
    "    Augment   = False, # Apply on-the-fly augmentation\n",
    "    Channels  = [0,] # How many projection IDs?\n",
    ")\n",
    "\n",
    "# Bring in the labels:\n",
    "cb.add_batch_filler(\n",
    "    datatype  = \"particle\",\n",
    "    producer  = \"queue_demo\",\n",
    "    name      = name+\"label\",\n",
    ")\n",
    "\n",
    "\n",
    "# Build up the data_keys:\n",
    "data_keys = {\n",
    "    'image': name + 'data',\n",
    "    'label': name + 'label'\n",
    "    }\n",
    "\n",
    "import json\n",
    "print(json.dumps(cb.get_config(), indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96794da",
   "metadata": {},
   "source": [
    "A lot of this configuration gets filled in for you, automatically, because larcv has defaults for every setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f979355",
   "metadata": {},
   "source": [
    "Next, we create a queue loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a782bcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from larcv.queueloader import queue_interface\n",
    "# from larcv.distributed_queue_interface import queue_interface # This is the multi-node, parallel version!\n",
    "\n",
    "queue_interface = queue_interface(random_access_mode=\"random_blocks\", seed=1234)\n",
    "queue_interface.no_warnings()\n",
    "\n",
    "# This gets the queue interface started:\n",
    "batch_size=4\n",
    "\n",
    "\n",
    "# Prepare data managers: (You can have more than one!  Train / test / anything)\n",
    "io_config = {\n",
    "    'filler_name' : name,\n",
    "    'filler_cfg'  : cb.get_config(),\n",
    "    'verbosity'   : 5,\n",
    "    'make_copy'   : False # This is if you want to make a copy of data in python\n",
    "}\n",
    "\n",
    "\n",
    "queue_interface.prepare_manager(name, io_config, batch_size, data_keys, color=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4ed166",
   "metadata": {},
   "source": [
    "Now that everything is configured, you can load data with the queue interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = queue_interface.fetch_minibatch_data(name, pop=True,fetch_meta_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbbc6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588c81cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the vertex locations of this data:\n",
    "print(data_dict['label'][\"_vtx\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc30209",
   "metadata": {},
   "source": [
    "If you call `prepare_next` before your data-fetching step ends, you will launch a C++ thread in the background to read the next batch of data while you're using this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815910bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue_interface.prepare_next(name) # prepare more data for the dataset tagged with `name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5483f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you call with `pop` = False, you get back the same data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567b52dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_data_dict = queue_interface.fetch_minibatch_data(name, pop=False,fetch_meta_data=True)\n",
    "print(re_data_dict['label'][\"_vtx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b21f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling with pop = True will discard this data after you use it - so you get it one more time,\n",
    "# but the next time is different.  In general, you usually want pop = True.\n",
    "re_data_dict = queue_interface.fetch_minibatch_data(name, pop=True,fetch_meta_data=True)\n",
    "queue_interface.prepare_next(name)\n",
    "print(re_data_dict['label'][\"_vtx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddac4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_data_dict = queue_interface.fetch_minibatch_data(name, pop=True,fetch_meta_data=True)\n",
    "queue_interface.prepare_next(name)\n",
    "print(re_data_dict['label'][\"_vtx\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0265be78",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "The queue interface provides a way to streamline data readback, letting you write a few lines of python to build a config, start a loader, and then you can get your dataset in one line per iteration.  More - and better - documentation is a work in progress and will be coming soon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07002ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
